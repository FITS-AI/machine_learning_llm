{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percobaan Awal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Language Model Implementation with TensorFlow\n",
    "\n",
    "## Import Library yang Diperlukan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing dan Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.char_to_idx: Dict = {}\n",
    "        self.idx_to_char: Dict = {}\n",
    "        self.vocab_size: int = 0\n",
    "        \n",
    "    def fit(self, text: str) -> None:\n",
    "        \"\"\"\n",
    "        Membuat vocabulary dari text\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text input\n",
    "        \"\"\"\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "        self.vocab_size = len(chars)\n",
    "        \n",
    "    def encode(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Mengubah text menjadi sequence of indices\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text input\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Array of indices\n",
    "        \"\"\"\n",
    "        return np.array([self.char_to_idx[ch] for ch in text])\n",
    "    \n",
    "    def decode(self, indices: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Mengubah sequence of indices menjadi text\n",
    "        \n",
    "        Args:\n",
    "            indices (np.ndarray): Array of indices\n",
    "            \n",
    "        Returns:\n",
    "            str: Decoded text\n",
    "        \"\"\"\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices])\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Menyimpan processor ke file\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path untuk menyimpan file\n",
    "        \"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'char_to_idx': self.char_to_idx,\n",
    "                'idx_to_char': self.idx_to_char,\n",
    "                'vocab_size': self.vocab_size\n",
    "            }, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'TextProcessor':\n",
    "        \"\"\"\n",
    "        Memuat processor dari file\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path file processor\n",
    "            \n",
    "        Returns:\n",
    "            TextProcessor: Instance processor yang dimuat\n",
    "        \"\"\"\n",
    "        processor = cls()\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            processor.char_to_idx = data['char_to_idx']\n",
    "            processor.idx_to_char = data['idx_to_char']\n",
    "            processor.vocab_size = data['vocab_size']\n",
    "        return processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation dengan TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLM(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 embedding_dim: int, \n",
    "                 rnn_units: int,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        \"\"\"\n",
    "        Inisialisasi TensorFlow Language Model\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Ukuran vocabulary\n",
    "            embedding_dim (int): Dimensi embedding\n",
    "            rnn_units (int): Jumlah unit RNN\n",
    "            dropout_rate (float): Rate untuk dropout\n",
    "        \"\"\"\n",
    "        super(TFLM, self).__init__()\n",
    "        \n",
    "        # Layer definitions\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                     return_sequences=True,\n",
    "                                     return_state=True,\n",
    "                                     recurrent_initializer='glorot_uniform')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, inputs: tf.Tensor, \n",
    "            states: Optional[tf.Tensor] = None, \n",
    "            training: bool = False) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass dari model\n",
    "        \n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input sequence\n",
    "            states (Optional[tf.Tensor]): Initial state\n",
    "            training (bool): Training mode flag\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[tf.Tensor, tf.Tensor]: Output dan final state\n",
    "        \"\"\"\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.dropout(x, training=training)\n",
    "        output, states = self.gru(x, initial_state=states)\n",
    "        x = self.dropout(output, training=training)\n",
    "        logits = self.dense(x)\n",
    "        return logits, states\n",
    "    \n",
    "    def initialize_states(self, batch_size: int) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Inisialisasi state untuk inference\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Batch size\n",
    "            \n",
    "        Returns:\n",
    "            tf.Tensor: Initial state\n",
    "        \"\"\"\n",
    "        return tf.zeros([batch_size, self.gru.units])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Manager\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, \n",
    "                 model: TFLM,\n",
    "                 processor: TextProcessor,\n",
    "                 checkpoint_dir: str = './training_checkpoints'):\n",
    "        \"\"\"\n",
    "        Inisialisasi training manager\n",
    "        \n",
    "        Args:\n",
    "            model (TFLM): Instance model\n",
    "            processor (TextProcessor): Text processor\n",
    "            checkpoint_dir (str): Directory untuk checkpoints\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "        self.history = {'loss': [], 'val_loss': []}\n",
    "        \n",
    "        # Setup optimizer dan loss\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True)\n",
    "            \n",
    "        # Setup checkpoint\n",
    "        self.checkpoint = tf.train.Checkpoint(\n",
    "            optimizer=self.optimizer,\n",
    "            model=self.model)\n",
    "            \n",
    "    @tf.function\n",
    "    def train_step(self, \n",
    "                  inputs: tf.Tensor, \n",
    "                  targets: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Single training step\n",
    "        \n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input batch\n",
    "            targets (tf.Tensor): Target batch\n",
    "            \n",
    "        Returns:\n",
    "            tf.Tensor: Loss value\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = self.model(inputs, training=True)\n",
    "            loss = self.loss(targets, predictions)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.model.trainable_variables))\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "    def train(self, \n",
    "              text: str,\n",
    "              epochs: int,\n",
    "              batch_size: int = 64,\n",
    "              seq_length: int = 100,\n",
    "              validation_split: float = 0.1,\n",
    "              log_every: int = 10):\n",
    "        \"\"\"\n",
    "        Training loop\n",
    "        \n",
    "        Args:\n",
    "            text (str): Training text\n",
    "            epochs (int): Number of epochs\n",
    "            batch_size (int): Batch size\n",
    "            seq_length (int): Sequence length\n",
    "            validation_split (float): Validation data ratio\n",
    "            log_every (int): Logging frequency\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        text_as_int = self.processor.encode(text)\n",
    "        char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "        sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "        \n",
    "        def split_input_target(chunk):\n",
    "            input_text = chunk[:-1]\n",
    "            target_text = chunk[1:]\n",
    "            return input_text, target_text\n",
    "            \n",
    "        dataset = sequences.map(split_input_target)\n",
    "        \n",
    "        # Split into train and validation\n",
    "        n_samples = len(list(dataset))\n",
    "        n_val = int(n_samples * validation_split)\n",
    "        \n",
    "        train_data = dataset.skip(n_val).shuffle(10000).batch(\n",
    "            batch_size, drop_remainder=True)\n",
    "        val_data = dataset.take(n_val).batch(\n",
    "            batch_size, drop_remainder=True)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            start = datetime.now()\n",
    "            \n",
    "            # Training\n",
    "            total_loss = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            for batch in train_data:\n",
    "                loss = self.train_step(batch[0], batch[1])\n",
    "                total_loss += loss\n",
    "                n_batches += 1\n",
    "                \n",
    "            train_loss = total_loss / n_batches\n",
    "            \n",
    "            # Validation\n",
    "            total_val_loss = 0\n",
    "            n_val_batches = 0\n",
    "            \n",
    "            for batch in val_data:\n",
    "                predictions, _ = self.model(batch[0], training=False)\n",
    "                val_loss = self.loss(batch[1], predictions)\n",
    "                total_val_loss += val_loss\n",
    "                n_val_batches += 1\n",
    "                \n",
    "            val_loss = total_val_loss / n_val_batches\n",
    "            \n",
    "            # Record history\n",
    "            self.history['loss'].append(train_loss.numpy())\n",
    "            self.history['val_loss'].append(val_loss.numpy())\n",
    "            \n",
    "            # Logging\n",
    "            if epoch % log_every == 0:\n",
    "                print(\n",
    "                    f'Epoch {epoch+1} | '\n",
    "                    f'Loss: {train_loss:.4f} | '\n",
    "                    f'Val Loss: {val_loss:.4f} | '\n",
    "                    f'Time: {datetime.now() - start}'\n",
    "                )\n",
    "                \n",
    "                # Save checkpoint\n",
    "                self.checkpoint.save(file_prefix=self.checkpoint_prefix)\n",
    "                \n",
    "    def generate_text(self, \n",
    "                     start_string: str,\n",
    "                     num_chars: int = 1000,\n",
    "                     temperature: float = 1.0) -> str:\n",
    "        \"\"\"\n",
    "        Generate text dari model\n",
    "        \n",
    "        Args:\n",
    "            start_string (str): String awal\n",
    "            num_chars (int): Jumlah karakter yang akan digenerate\n",
    "            temperature (float): Sampling temperature\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated text\n",
    "        \"\"\"\n",
    "        # Converting start string to numbers\n",
    "        input_eval = self.processor.encode(start_string)\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        \n",
    "        # Empty string to store result\n",
    "        text_generated = []\n",
    "        \n",
    "        # Reset states\n",
    "        states = None\n",
    "        \n",
    "        for _ in range(num_chars):\n",
    "            predictions, states = self.model(\n",
    "                input_eval, states=states, training=False)\n",
    "                \n",
    "            # Remove batch dimension\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "            \n",
    "            # Using temperature for sampling\n",
    "            predictions = predictions / temperature\n",
    "            predicted_id = tf.random.categorical(\n",
    "                predictions, num_samples=1)[-1,0].numpy()\n",
    "                \n",
    "            # Pass the predicted char as next input\n",
    "            input_eval = tf.expand_dims([predicted_id], 0)\n",
    "            \n",
    "            text_generated.append(\n",
    "                self.processor.idx_to_char[predicted_id])\n",
    "                \n",
    "        return start_string + ''.join(text_generated)\n",
    "        \n",
    "    def plot_history(self):\n",
    "        \"\"\"\n",
    "        Plot training history\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history['loss'], label='Training Loss')\n",
    "        plt.plot(self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def save_model(self, model_dir: str):\n",
    "        \"\"\"\n",
    "        Menyimpan model dan processor\n",
    "        \n",
    "        Args:\n",
    "            model_dir (str): Directory untuk menyimpan model\n",
    "        \"\"\"\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "            \n",
    "        # Save model\n",
    "        self.model.save(os.path.join(model_dir, 'model'))\n",
    "        \n",
    "        # Save processor\n",
    "        self.processor.save(\n",
    "            os.path.join(model_dir, 'processor.pkl'))\n",
    "            \n",
    "    @classmethod\n",
    "    def load_model(cls, \n",
    "                  model_dir: str,\n",
    "                  checkpoint_dir: str = './training_checkpoints') -> 'ModelTrainer':\n",
    "        \"\"\"\n",
    "        Memuat model dan processor\n",
    "        \n",
    "        Args:\n",
    "            model_dir (str): Directory model\n",
    "            checkpoint_dir (str): Directory checkpoints\n",
    "            \n",
    "        Returns:\n",
    "            ModelTrainer: Instance trainer yang dimuat\n",
    "        \"\"\"\n",
    "        # Load processor\n",
    "        processor = TextProcessor.load(\n",
    "            os.path.join(model_dir, 'processor.pkl'))\n",
    "            \n",
    "        # Load model\n",
    "        model = tf.keras.models.load_model(\n",
    "            os.path.join(model_dir, 'model'))\n",
    "            \n",
    "        # Create trainer instance\n",
    "        trainer = cls(model, processor, checkpoint_dir)\n",
    "        \n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contoh Penggunaan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \n",
    "concerned with the interactions between computers and human language, in particular how to program computers to \n",
    "process and analyze large amounts of natural language data.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize processor and create vocabulary\n",
    "processor = TextProcessor()\n",
    "processor.fit(text)\n",
    "\n",
    "# Create model\n",
    "model = TFLM(\n",
    "    vocab_size=processor.vocab_size,\n",
    "    embedding_dim=256,\n",
    "    rnn_units=512,\n",
    "    dropout_rate=0.2\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer(model, processor)\n",
    "\n",
    "# Training\n",
    "trainer.train(\n",
    "    text=text,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    seq_length=100,\n",
    "    validation_split=0.1,\n",
    "    log_every=5\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "trainer.plot_history()\n",
    "\n",
    "# Generate some text\n",
    "generated_text = trainer.generate_text(\n",
    "    start_string=\"Natural\",\n",
    "    num_chars=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "# Save model\n",
    "trainer.save_model('./saved_model')\n",
    "\n",
    "# Load model (untuk penggunaan nanti)\n",
    "loaded_trainer = ModelTrainer.load_model('./saved_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
